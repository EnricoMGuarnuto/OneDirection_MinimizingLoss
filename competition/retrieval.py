import os
import argparse
import yaml
import torch
import numpy as np
from torchvision import models
from PIL import Image
from tqdm import tqdm
import json
import timm
import torch.nn as nn
import open_clip
import torchvision.models as tv_models
from submit import submit  # importa la funzione ufficiale
from torch.utils.data import Dataset, DataLoader


# Function provided by organizers (placeholder)
# def submit(results_dict, group_name):
#     # Replace with actual submission logic
#     print(f"üìù Submitting results for group: {group_name}")
#     print(f"üî¢ {len(results_dict)} queries processed.")
#     # You can replace this with the real submit() function provided on competition day
#     with open(f"{group_name}_submission.json", "w") as f:
#         json.dump(results_dict, f, indent=2)
#     print(f"‚úÖ Submission dictionary saved as {group_name}_submission.json")


class ImageDataset(Dataset):
    def __init__(self, image_paths, root_folder, transform):
        self.image_paths = image_paths
        self.root_folder = root_folder
        self.transform = transform

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        rel_path = self.image_paths[idx]
        img_path = os.path.join(self.root_folder, rel_path)
        img = Image.open(img_path).convert('RGB')
        return self.transform(img), rel_path

def load_model(cfg, device):
    name = cfg['model']['name']
    source = cfg['model'].get('source', 'torchvision')
    pretrained = cfg['model'].get('pretrained', True)
    checkpoint_path = cfg['model'].get('checkpoint_path', '')
    num_classes = cfg['model'].get('num_classes', 1000)

    if name == 'moco_resnet50':
        model = tv_models.resnet50(pretrained=False)
        model.fc = nn.Identity()  # ‚ö† rimuoviamo subito la testa (output 1000) ‚Üí embedding 2048
        checkpoint = torch.load(checkpoint_path, map_location=device)
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
            new_state_dict = {k.replace('module.encoder_q.', ''): v for k, v in state_dict.items() if k.startswith('module.encoder_q')}
            model.load_state_dict(new_state_dict, strict=False)
            print(f"‚úÖ Loaded MoCo v2 ResNet50 from {checkpoint_path}")
        else:
            # directly a state_dict (from fine-tuned weights)
            filtered_state_dict = {k: v for k, v in checkpoint.items() if not k.startswith('fc.')}
            model.load_state_dict(filtered_state_dict, strict=False)
            print(f"‚úÖ Loaded fine-tuned weights from {checkpoint_path}")


    elif source == 'open_clip':
        model, _, _ = open_clip.create_model_and_transforms(name, pretrained=pretrained)
        model = model.visual  # only visual encoder
        if checkpoint_path:
            state_dict = torch.load(checkpoint_path, map_location=device)
            model.load_state_dict(state_dict, strict=False)
            print(f"‚úÖ Loaded custom weights from {checkpoint_path}")
        else:
            print(f"‚úÖ Loaded {name} with pretrained weights from open_clip")

    elif source == 'timm':
        model = timm.create_model(name, pretrained=pretrained, num_classes=num_classes)
        if checkpoint_path:
            state_dict = torch.load(checkpoint_path, map_location=device)
            filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.') and not k.startswith('classifier.')}
            model.load_state_dict(filtered_state_dict, strict=False)
            print(f"‚úÖ Loaded custom backbone weights from {checkpoint_path}")
        model.reset_classifier(0, '')  # remove classifier head
        print(f"‚úÖ Loaded {name} from timm")

    else:
        model_fn = getattr(tv_models, name)
        model = model_fn(pretrained=pretrained)
        if checkpoint_path:
            state_dict = torch.load(checkpoint_path, map_location=device)
            filtered_state_dict = {k: v for k, v in state_dict.items() if not k.startswith('fc.') and not k.startswith('classifier.')}
            model.load_state_dict(filtered_state_dict, strict=False)
            print(f"‚úÖ Loaded custom backbone weights from {checkpoint_path}")
        if hasattr(model, 'fc'):
            model.fc = nn.Identity()
        elif hasattr(model, 'classifier'):
            model.classifier = nn.Identity()
        print(f"‚úÖ Loaded {name} from torchvision")

    return model.to(device)

def get_image_paths(folder):
    all_files = []
    for root, _, files in os.walk(folder):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                all_files.append(file)
    return sorted(all_files)

def extract_embeddings(model, image_paths, root_folder, device, img_size, norm_mean, norm_std, source, name, pretrained_tag, batch_size):
    if source == 'open_clip':
        _, _, preprocess = open_clip.create_model_and_transforms(name, pretrained=pretrained_tag)
    else:
        preprocess = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=norm_mean, std=norm_std)
        ])

    dataset = ImageDataset(image_paths, root_folder, preprocess)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8)

    embeddings = {}
    model.eval()
    with torch.no_grad():
        for batch_imgs, batch_paths in tqdm(dataloader, desc=f"Extracting features from {root_folder}"):
            batch_imgs = batch_imgs.to(device)
            feats = model(batch_imgs).cpu().numpy()
            feats = feats / np.linalg.norm(feats, axis=1, keepdims=True)
            for path, feat in zip(batch_paths, feats):
                embeddings[path] = feat
    return embeddings

def compute_topk(query_embeddings, gallery_embeddings, k):
    gallery_files = list(gallery_embeddings.keys())
    gallery_feats = np.stack(list(gallery_embeddings.values()))
    results = {}
    for query_file, query_feat in tqdm(query_embeddings.items(), desc="Computing retrieval"):
        sims = np.dot(gallery_feats, query_feat)
        topk_idx = np.argsort(sims)[-k:][::-1]
        topk_files = [gallery_files[i] for i in topk_idx]
        results[query_file] = topk_files
    return results

def save_json(results, output_path):
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"‚úÖ Saved retrieval results to {output_path}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, required=True, help='Path to config YAML file')
    parser.add_argument('--group', type=str, required=True, help='Group name for submission')
    args = parser.parse_args()

    with open(args.config, 'r') as f:
        cfg = yaml.safe_load(f)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = load_model(cfg, device)

    gallery_dir = cfg['data']['gallery_dir']
    query_dir = cfg['data']['query_dir']
    img_size = cfg['data'].get('img_size', 224)
    batch_size = cfg['data'].get('batch_size', 1)
    norm_mean = cfg['data'].get('normalization', {}).get('mean', [0.485, 0.456, 0.406])
    norm_std = cfg['data'].get('normalization', {}).get('std', [0.229, 0.224, 0.225])
    top_k = cfg['retrieval'].get('top_k', 10)
    output_json = cfg['retrieval'].get('output_json', 'retrieval_results.json')

    source = cfg['model'].get('source', 'torchvision')
    name = cfg['model']['name']
    
    pretrained = cfg['model']['pretrained']

    gallery_paths = get_image_paths(gallery_dir)
    query_paths = get_image_paths(query_dir)

    gallery_embeddings = extract_embeddings(model, gallery_paths, gallery_dir, device, img_size, norm_mean, norm_std, source, name, pretrained, batch_size)
    query_embeddings = extract_embeddings(model, query_paths, query_dir, device, img_size, norm_mean, norm_std, source, name, pretrained, batch_size)

    retrieval_results = compute_topk(query_embeddings, gallery_embeddings, top_k)
    save_json(retrieval_results, output_json)

    submit(retrieval_results, args.group)

if __name__ == '__main__':
    main()